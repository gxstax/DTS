######################################################################################################
#################################           kafka 配置           ######################################
###################  配置方式：每一类配置中[]中的内容和原生kafka API保持一致即可   ############################
###################  1.common-client-config                                      #####################
###################  2.producer-config                                           #####################
###################  3.admin-client-config                                       #####################
###################  4.consumer-config (这里只是一个示例，消费者端在各自的客户业务端实现) #####################
######################################################################################################

##------------------ 1.common-client-config: kafka 公共配置参数  --------------------------------------##
##   bootstrap.servers: 服务地址                                                                      ##
##---------------------------------------------------------------------------------------------------##
ant.data-transfer-center.kafka.common.common-client-config.[bootstrap.servers]=${bootstrap.servers}



##------------------ 2.producer-config: kafka 生产者端参数配置说明 ------------------------------------- ##
##                                                                                                   ##
##   compression.type:  压缩算法 秉承规则: producer端压缩,broker端保持、Consumer端解压缩 保证压缩格式的一致性   ##
##   partitioner.class: 分区过滤器                                                                     ##
##   key.serializer:    消息 key序列化格式，                                                            ##
##   value.serializer   消息体 序列化格式                                                               ##
##   acks=all:          所有副本都接收到消息，消息才算'已提交' 防止消息丢失                                   ##
##   retries:           重试次数，producer 发送失败重试次数，防止网络抖动丢失消息                             ##
##   enable.idempotence: 消息是否幂等，单独配置只保证单分区幂等，如果实现事务，需要写事务型生产者语法              ##
##   transactional.id:   事务id，事务型 producer 参数                                                   ##
##   max.block.ms:                                                                                   ##
##   batch.size:         每批数据缓存最大字节数                                                          ##
##   linger.ms:          每批次缓存时间                                                                ##
##---------------------------------------------------------------------------------------------------##
ant.data-transfer-center.kafka.producer.producer-config.[compression.type]=lz4
ant.data-transfer-center.kafka.producer.producer-config.[partitioner.class]=com.ant.dtc.output.kafka.producer.partitioner.KafkaPartitioner
ant.data-transfer-center.kafka.producer.producer-config.[key.serializer]=org.apache.kafka.common.serialization.StringSerializer
ant.data-transfer-center.kafka.producer.producer-config.[value.serializer]=org.apache.kafka.common.serialization.StringSerializer
ant.data-transfer-center.kafka.producer.producer-config.[acks]=all
ant.data-transfer-center.kafka.producer.producer-config.[retries]=2
ant.data-transfer-center.kafka.producer.producer-config.[enable.idempotence]=false
# ant.data-transfer-center.kafka.producer.producer-config.[transactional.id]=ant
ant.data-transfer-center.kafka.producer.producer-config.[batch.size]=524288
ant.data-transfer-center.kafka.producer.producer-config.[max.block.ms]=5000
ant.data-transfer-center.kafka.producer.producer-config.[linger.ms]=0



##-------------------- 3.admin-client-config: kafka 管理端配置参数   ------------------------------------##
##   request.timeout.ms： 请求超时时间                                                                   ##
##----------------------------------------------------------------------------------------------------##
ant.data-transfer-center.kafka.admin.admin-client-config.[request.timeout.ms]=600000



##-------------------- 4.consumer-config: kafka 消费者端配置  -------------------------------------------##
##  max.poll.records:      每次最多拉取多少条数据                                                          ##
##  enable.auto.commit:    是否开启消息自动确认                                                            ##
##  fetch.min.bytes:       接收字节数(该值表示broker端至少累积多大的数据量，客户端才拉取)；                       ##
##                         如果你追求低延迟，这里可以设置为1，表示只要broker端有数据就拉取;                       ##
##  heartbeat.interval.ms: 心跳检测包发送频率                                                              ##
##  session.timeout.ms:    session.timeout.ms >= 3 * heartbeat.interval.ms                              ##
##                         (意思是保证 Consumer 实例在被判定为“dead”之前，能够发送至少 3 轮的心跳请求)            ##
##  max.poll.interval.ms:  你的业务要在这个时间内处理完 max.poll.records 设置的消息条数，                       ##
##                         如果你的业务处理相对比较慢，可以适当调大该参数，避免Rebalance                         ##
##  key.deserializer:      消息 key序列化格式，                                                           ##
##  value.deserializer:    消息体序列化格式，                                                             ##
##  group.id:              消息者组id                                                                   ##
##----------------------------------------------------------------------------------------------------##
ant.data-transfer-center.kafka.consumer.consumer-config.[max.poll.records]=2000
ant.data-transfer-center.kafka.consumer.consumer-config.[enable.auto.commit]=false
ant.data-transfer-center.kafka.consumer.consumer-config.[fetch.min.bytes]=1
ant.data-transfer-center.kafka.consumer.consumer-config.[session.timeout.ms]=6000
ant.data-transfer-center.kafka.consumer.consumer-config.[heartbeat.interval.ms]=2000
ant.data-transfer-center.kafka.consumer.consumer-config.[max.poll.interval.ms]=360000
ant.data-transfer-center.kafka.consumer.consumer-config.[key.deserializer]=org.apache.kafka.common.serialization.StringDeserializer
ant.data-transfer-center.kafka.consumer.consumer-config.[value.deserializer]=org.apache.kafka.common.serialization.StringDeserializer
ant.data-transfer-center.kafka.consumer.consumer-config.[group.id]=ant
